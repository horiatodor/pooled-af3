# Example Pooled-AlphaFold3 workflow

This repository describes the generation of "pools" of proteins and the analysis of their predicted structures as performed in:

https://www.biorxiv.org/content/10.1101/2025.07.01.662654v2

This code can be vastly improved and should be used to replicate the results of that manuscript and as inspiration for future code, not necessarily as the "gold-standard" of pooled-AlphaFold3 code. 


## This code will help you take a fasta file of protein sequences and generate a set of "pools" suitable for running pooled-AlphaFold3 using the free webserver. 

1) Read in the pooling code:

    ```
    source("all_by_all_af34_v2.R")
    ```

2) Read in the protein sequences using the seqinr package:
   
    ```
    mgen_proteins <- seqinr::read.fasta("mgen_proteins.txt", forceDNAtolower = FALSE, as.string = TRUE)
    ```

3) Generate the pools of the proteins we want using the functions we just loaded. This is not computationally efficient code and we suggest not running it for genomes much larger than ~1600 genes. 

    ```
    pools_to_write <- all_by_all(mgen_proteins)
    ```

4) Read in the JSON writer:
   This is not a great json writer - surely there are better ones available. 

    ```
    source("json_writer_4_8.R")
    ```

5) Set a prefix for the jobs and an output directory

    ```
    name_prefix <- "mgen" 
    output_dir <- "test/"
    ```

6) Take the pools we generated, parse them, and write them (batch size indicates how many runs to put in each json).

    ```
    parsed_jobs <- job_parser(pools_to_write[[1]], name_prefix, batch_size = 30)
    write_files(parsed_jobs, name_prefix, output_dir)
    ```

## Then, assuming that you've run and downloaded all of the sequences, this code will show you how to do a first pass analysis. 

1) We will concatenate all of the description files generated by the code (above) into as single matrix with 3 columns: pool name, proteins in pool (separated by ::::), and number of proteins in pool.

    ```
    #read in all of the description files and concetenate them
    description_files <- paste0("/Volumes/Samsung_T5/AF3 backup/job_info/", list.files("/Volumes/Samsung_T5/AF3 backup/job_info/"))
    description_files <- description_files[-grep("matrix_of_pairs", description_files)]
    list_of_descriptions <- list()
    
    for (i in seq_along(description_files)){
      list_of_descriptions[[i]] <- read.csv(description_files[i], row.names = 1, stringsAsFactors = FALSE)
    }
    
    all_descriptions <- do.call("rbind", list_of_descriptions)  
    all_descriptions[,1] <- apply(do.call("rbind", strsplit(all_descriptions[,1], split = "__")),1,paste, collapse = "_")
    
    rm(list_of_descriptions)
    ```

2) Read in the analysis code
    
    ```
    source("analysis_functions.R")
    ```

3) Read in the ipTMs. This will look in the folder for zip file, open each zip file, read in the "summary_confidences" file for all 5 models, and average the ipTMs. A few notes:

   -This code will ignore all jobs that are not in the descriptions file. Not matching the names is a common failure point.
   -You can also ask for it to return PAE and SD (of ipTM diffusion samples).

    ```
    iptms_and_paes <- get_list_of_iptm(afolder_of_zip = "/Volumes/Samsung_T5/AF3 backup/AF3_completed_zips/", 
                                       description = all_descriptions, return_paes = FALSE, return_sd = FALSE)
    ```

4) [optional] If your downloading and/or running of jobs was perfect (each job run once), you can skip this step. If you have duplicates, this step will average the ipTM matrices for all jobs with the same name.

   ```
   temp2 <- resolve_duplicates(iptms_and_paes) 

   ```
   
5) Apply the size correction. The code here uses a robust linear regression (robustbase::lmrob) between the square-root of the sum size of two proteins and their ipTM to produce an expected ipTM, and then subtracts the expected ipTM from the observed ipTM to generate a "size corrected ipTM". Parameters vary mildly between data sets but they should be in the neighborhood of: expected_ipTM = -0.036255571 + 0.004470512*sqrt(aa_in_protein1 + aa_in_protein2). For datasets that may be too small to robustly estimate these parameters, they can be explicitly set.

    ```
    list_of_iptm_size_corr <- size_correct(temp2[[1]], mgen_proteins)[[1]]

    or

    list_of_iptm_size_corr <- size_correct(temp2[[1]], mgen_proteins, coefs = c(-0.0363, 0.00447)[[1]]
    ```
  
  
6) Now lets generate the matrix of all-by-all ipTMs.

    ```
    the_final_data <- make_matrix_all_by_all(list_of_iptm_size_corr, mgen_proteins, resolve_reps = "mean")
    ```
  
    This will take a few minutes to run depending on the size of the organism. ```resolve_reps``` describes how the ipTMs of protein pairs that appear in multiple pools should be handled. Options include ```mean median max min``` and a few others. 
